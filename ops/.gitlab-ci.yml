# Notes: Update Composer's PyPi packages to reflect changes in the tools codebase, if necessary
workflow:
  rules:
    # Prevent duplicate merge request pipelines (until they work with protected vars)
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: never
    - when: always

variables:
  # CI_COMMIT_TAG: $CI_COMMIT_SHORT_SHA
  # NAME CONSTANTS
  SERVICE_NAME: "analytics-tools"
  POSTFIX: "$CI_COMMIT_SHORT_SHA"
  UNITTEST_DAG: "at_unittest_dag_$CI_COMMIT_SHORT_SHA"
  INTEGRATIONTEST_DAG: "at_inttest_dag_$CI_COMMIT_SHORT_SHA"
  # DIRECTORIES
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  DAGS_DIR: "$CI_PROJECT_DIR/tests/test_harness/dags"
  TEST_HARNESS_DIR: "$CI_PROJECT_DIR/tests/test_harness"
  TEST_PIPELINE_DIR: "$TEST_HARNESS_DIR/testing_pipeline"
  # CLOUD-BASED DIRECTORIES
  DAGS_URI: "$COMPOSER_BUCKET/dags"
  PLUGINS_URI: "$COMPOSER_BUCKET/plugins"
  DATA_URI: "$COMPOSER_BUCKET/data"
  CI_URI: "$COMPOSER_BUCKET/ci"
  CONFIG_URI: "$COMPOSER_BUCKET/config"

stages:
  - code_quality
  - package_management
  - deploy
  - verify

code_quality:
  stage: code_quality
  image: python:latest
  rules:
    - if: $CI_PIPELINE_SOURCE == "push"
  allow_failure: true
  before_script:
    - pip install pylint
  script:
    - pylint data_tools/*.py | tee $CI_PROJECT_DIR/${CI_COMMIT_BRANCH}_lint.txt
  artifacts:
    paths:
      - $CI_PROJECT_DIR/${CI_COMMIT_BRANCH}_lint.txt
    when: on_failure
    expire_in: 1 week

delete_package:test:
  stage: package_management
  image: cdue/curl-jq:latest
  allow_failure: true
  rules:
    # ONLY RUN IF THE BRANCH IS NOT MASTER AND IF THE TOOLS CODE CHANGES
    - if: $CI_COMMIT_BRANCH != "master"
      changes:
        - data_tools/**/*
      allow_failure: true
  script:
    # PULL THE LATEST analytics-tools-test PACKAGE
    - curl --header "PRIVATE-TOKEN:$TWINE_PASSWORD" "https://gitlab.ops.canceriq.com/api/v4/projects/226/packages?package_name=analytics-tools-test" >> pk.json
    # GET THE ID OF THE LATEST analytics-tools-test PACKAGE
    - cat pk.json
    - pkid=$(jq '.[].id' pk.json)
    - echo $pkid
    # DELETE THE LATEST analytics-tools-test PACKAGE
    - curl --request DELETE --header "PRIVATE-TOKEN:$TWINE_PASSWORD" "https://gitlab.ops.canceriq.com/api/v4/projects/226/packages/${pkid}"

create_package:test:
  stage: package_management
  image: python:3.9.18-slim
  dependencies:
    - delete_package:test
  rules:
    # ONLY RUN IF THE BRANCH IS NOT MASTER AND IF THE TOOLS CODE CHANGES
    - if: $CI_COMMIT_BRANCH != "master"
      changes:
        - data_tools/**/*
      allow_failure: true
  before_script:
    - pip install -r requirements_ci.txt
    - pip install twine
  script:
    # COPY SETUP.PY FOR TEST VERSION OF TOOLS
    - cp setup.py setup_test.py
    # RENAME tools refernces to tools_test
    - sed -i 's/name="data_tools"/name="data_tools_test"/g' setup_test.py
    # CREATE BUILD FOR ANALYTICS TOOLS TEST
    - python setup_test.py sdist bdist_wheel
    # DEPLOY ANALYTICS TOOLS TEST TO PACKAGE REGISTRY
    # CHECK THAT THE VERSION DOESN'T ALREADY EXIST
    - TWINE_PASSWORD=$TWINE_PASSWORD TWINE_USERNAME=$TWINE_USERNAME twine upload --skip-existing --repository-url https://gitlab.ops.canceriq.com/api/v4/projects/226/packages/pypi dist/*
    - exit 0
  after_script:
    - ls
  allow_failure:
    exit_codes:
      - 0

deploy:unit:
  stage: deploy
  image: google/cloud-sdk:slim
  rules:
    - if: $CI_COMMIT_BRANCH != "master"
    - changes:
        # only run if tools code changes
        - tests/*
  before_script:
    # ACTIVATE PROD SERVICE ACCOUNT
    - gcloud auth activate-service-account --key-file=$SERVICE_ACCOUNT_KEY
    # INSTALL ZIP, KUBECTL, GKE AUTH PLUGIN
    - apt-get install -y zip
    - apt-get install kubectl
    - apt-get install google-cloud-sdk-gke-gcloud-auth-plugin
  script:
    # ADD COMMIT SHA AS POSTFIX TO (AND WITHIN) DAG FILES
    - find tests/test_harness/dags -type f -print0 | xargs -0 sed -i "s/postfix/$POSTFIX/g"
    # MOVE NECESSARY FILES/DIRS TO STAGING AREA
    - cp tests/test_harness/dags/tools_unittest_dag.py .
    - cp -r tests/test_harness/testing_pipeline .
    - cp -r tests/test_harness/configs .
    - cp -r tests/test_harness/testing_pipeline/scripts .
    # GENERATE CLUSTER NAME AND BRANCH NAME
    - echo at-unit-dag-$[ $RANDOM % 420 + 1 ] >> configs/cluster_name.txt
    - echo $CI_COMMIT_BRANCH >> configs/branch_name.txt
    # COMPRESS DAG (AND SUPPORTING FILES)
    - zip -r $UNITTEST_DAG.zip testing_pipeline *_dag.py
    - ls
    # MOVE THE STAGED FILES TO THEIR APPROPIATE BUCKETS
    - gsutil cp -r *.zip $DAGS_URI
    - gsutil cp -r configs testing_pipeline $DAGS_URI/$UNITTEST_DAG

verify:unit:
  stage: verify
  image: google/cloud-sdk
  needs:
    - job: deploy:unit
      artifacts: true
  timeout: 1 hour
  script:
    - gcloud auth activate-service-account --key-file=$SERVICE_ACCOUNT_KEY
    - chmod +x postpipeline.sh
    - bash postpipeline.sh $CI_URI $UNITTEST_DAG

# deploy:integration:
#   stage: deploy
#   image: google/cloud-sdk:slim
#   rules:
#     - if: $CI_PIPELINE_SOURCE == "merge_request_event"
#   before_script:
#     # ACTIVATE PROD SERVICE ACCOUNT
#     - gcloud auth activate-service-account --key-file=$SERVICE_ACCOUNT_KEY
#     # INSTALL ZIP, KUBECTL, GKE AUTH PLUGIN
#     - apt-get install -y zip
#     - apt-get install kubectl
#     - apt-get install google-cloud-sdk-gke-gcloud-auth-plugin
#   script:
#     # ADD COMMIT SHA AS POSTFIX TO (AND WITHIN) DAG FILES
#     - find tests/test_harness/dags -type f -print0 | xargs -0 sed -i "s/postfix/$POSTFIX/g"
#     # MOVE NECESSARY FILES/DIRS TO STAGING AREA
#     - cp -r tests/test_harness/dags/tools_integrationtest_dag.py .
#     - cp -r tests/test_harness/testing_pipeline .
#     - cp -r tests/test_harness/configs .
#     - cp -r tests/test_harness/testing_pipeline/scripts .
#     # GENERATE CLUSTER NAME AND BRANCH NAME
#     - echo at-int-dag-$[ $RANDOM % 420 + 1 ] >> configs/cluster_name.txt
#     - echo $CI_COMMIT_BRANCH >> configs/branch_name.txt
#     # COMPRESS DAG (AND SUPPORTING FILES)
#     - zip -r at_dag_${POSTFIX}.zip testing_pipeline *_dag.py
#     - ls
#     # MOVE THE STAGED FILES TO THEIR APPROPIATE BUCKETS
#     - gsutil cp -r *.zip $DAGS_URI
#     - gsutil cp -r configs testing_pipeline $DATA_URI/at_dag_${POSTFIX}

# verify:integration:
#   stage: verify
#   image: google/cloud-sdk
#   needs:
#     - job: deploy:integration
#       artifacts: true
#   timeout: 1 hour
#   script:
#     - gcloud auth activate-service-account --key-file=$SERVICE_ACCOUNT_KEY
#     - chmod +x postpipeline.sh
#     - bash postpipeline.sh $CI_URI $INTEGRATIONTEST_DAG

create_package:production:
  stage: package_management
  image: python:latest
  rules:
    # RUN IN THE EVENT OF MERGE REQUEST OR PUSH TO MASTER
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == "master"
  before_script:
    # INSTALL TWINE
    - pip install twine
  script:
    # CREATE BUILD FOR ANALYTICS TOOLS TEST
    - python setup.py sdist bdist_wheel
    # DEPLOY ANALYTICS TOOLS TEST TO PACKAGE REGISTRY
    # CHECK THAT THE VERSION DOESN'T ALREADY EXIST
    - TWINE_PASSWORD=$TWINE_PASSWORD TWINE_USERNAME=$TWINE_USERNAME twine upload --skip-existing --repository-url https://gitlab.ops.canceriq.com/api/v4/projects/226/packages/pypi dist/*
    - exit 1
  after_script:
    - ls
  allow_failure:
    exit_codes:
      - 1
